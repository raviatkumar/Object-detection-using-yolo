{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7TED5VjPVD9rUCsE4uAJR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement: Automated Object Detection and Cropping in Video Files\n",
        "\n",
        "### Objective: Develop a solution that processes video files in .mp4 format to automatically detect objects in each frame and save the cropped images of these objects into a designated folder. The system should handle videos of a minimum length of 5 to 10 minutes and ensure that all detected objects are saved correctly and organized in a folder structure for easy comparison with the original input video."
      ],
      "metadata": {
        "id": "TLdpKS9YIR_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My8j_0fkKFRo",
        "outputId": "3eb1426d-8d21-49de-fd75-f5a84014984e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.66)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.5.82)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "uD9KoN4mKHqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weights path\n",
        "weights_path = '/content/drive/MyDrive/Intern assignment/Object detection/YOLO/yolov8s.pt'"
      ],
      "metadata": {
        "id": "baPkU5ELKHvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and save the YOLOv8 model\n",
        "model = YOLO('yolov8s.pt')  # Downloading the YOLOv8 small model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqKtlU-4KNjX",
        "outputId": "6966e25d-6255-427a-edcf-15823a52d4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21.5M/21.5M [00:00<00:00, 56.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights\n",
        "model.save(weights_path)\n",
        "print(f\"YOLOv8 weights saved to {weights_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvEKIXI3KNo8",
        "outputId": "a5eed61e-b9bd-4a95-d4bf-125ddc78bc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv8 weights saved to /content/drive/MyDrive/Intern assignment/Object detection/YOLO/yolov8s.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###import library"
      ],
      "metadata": {
        "id": "CzLlJwf-If-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import pandas\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "LP4vTDmVKNup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount drive"
      ],
      "metadata": {
        "id": "d-bqSPnKIkJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmgbq_ztHjDO",
        "outputId": "4a04f3d2-5b3f-4035-8f17-25fc01025514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "weights_path = '/content/drive/MyDrive/Intern assignment/Object detection/YOLO/yolov8s.pt'\n",
        "video_path = '/content/drive/MyDrive/Intern assignment/Object detection/supermarkets.mp4'\n",
        "output_folder = '/content/drive/MyDrive/Intern assignment/Object detection/cropped_objects'"
      ],
      "metadata": {
        "id": "P3oX1f7CHkc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8 model\n",
        "model = YOLO(weights_path)"
      ],
      "metadata": {
        "id": "iBPggnvWHkgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save cropped objects from video\n",
        "def save_cropped_objects(video_path, output_folder, num_frames=10):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = [int(i * frame_count / num_frames) for i in range(num_frames)]\n",
        "\n",
        "    for idx, frame_idx in enumerate(frame_indices):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Perform object detection\n",
        "        results = model(frame)\n",
        "\n",
        "        # Process results\n",
        "        for result in results:\n",
        "            # Extract bounding boxes and class names\n",
        "            for detection in result.boxes:\n",
        "                # Convert tensors to native Python data types\n",
        "                xyxy = detection.xyxy.tolist()  # Convert tensor to list\n",
        "                xmin, ymin, xmax, ymax = map(int, xyxy[0])\n",
        "                conf = float(detection.conf.item())\n",
        "                class_id = int(detection.cls.item())\n",
        "                class_name = model.names[class_id]\n",
        "\n",
        "                # Draw bounding box and label\n",
        "                color = (0, 255, 0)  # Green\n",
        "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "                label = f'{class_name} {conf:.2f}'\n",
        "                cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "                # Save cropped image\n",
        "                cropped_img = frame[ymin:ymax, xmin:xmax]\n",
        "                output_path = os.path.join(output_folder, f'frame_{idx+1}_object_{class_id}_{class_name}.jpg')\n",
        "                cv2.imwrite(output_path, cropped_img)\n",
        "\n",
        "        # Save the frame with bounding boxes and labels\n",
        "        frame_with_boxes_path = os.path.join(output_folder, f'frame_{idx+1}_with_boxes.jpg')\n",
        "        cv2.imwrite(frame_with_boxes_path, frame)\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "4OAu-b__Hkje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save cropped objects and frames with bounding boxes\n",
        "save_cropped_objects(video_path, output_folder, num_frames=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luJCgrkrHkmZ",
        "outputId": "02a1baa5-a488-4c71-e1a1-5973c7e0d462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 1 person, 1 cup, 458.5ms\n",
            "Speed: 4.3ms preprocess, 458.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 bottle, 2 cups, 1 hot dog, 1 dining table, 398.3ms\n",
            "Speed: 4.0ms preprocess, 398.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 4 oranges, 627.9ms\n",
            "Speed: 10.2ms preprocess, 627.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 bowls, 2 apples, 3 oranges, 1 carrot, 411.0ms\n",
            "Speed: 4.1ms preprocess, 411.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 apples, 403.5ms\n",
            "Speed: 4.9ms preprocess, 403.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 2 cups, 450.2ms\n",
            "Speed: 5.7ms preprocess, 450.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 bottle, 6 cups, 2 chairs, 432.9ms\n",
            "Speed: 6.6ms preprocess, 432.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 bottle, 1 wine glass, 1 cup, 1 dining table, 1 book, 429.0ms\n",
            "Speed: 5.7ms preprocess, 429.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 3 vases, 400.3ms\n",
            "Speed: 3.4ms preprocess, 400.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 potted plant, 401.4ms\n",
            "Speed: 5.8ms preprocess, 401.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    }
  ]
}